name: Enterprise-Scale Migration Testing

on:
  # Trigger on pull requests to main branch
  pull_request:
    branches: [ main, develop ]
    paths:
      - 'backend/**'
      - 'src/**'
      - 'scripts/testing/**'
      - '.github/workflows/**'
  
  # Trigger on pushes to main branch
  push:
    branches: [ main ]
  
  # Scheduled enterprise-scale testing (nightly)
  schedule:
    # Run at 2 AM UTC daily
    - cron: '0 2 * * *'
  
  # Manual trigger with parameters
  workflow_dispatch:
    inputs:
      test_level:
        description: 'Test level to execute'
        required: true
        default: 'integration'
        type: choice
        options:
          - unit
          - integration
          - stress
          - full
      
      parallel_execution:
        description: 'Enable parallel test execution'
        required: false
        default: true
        type: boolean
      
      performance_baseline:
        description: 'Establish new performance baseline'
        required: false
        default: false
        type: boolean

env:
  # Test environment configuration
  NODE_VERSION: '18'
  GO_VERSION: '1.21'
  MYSQL_VERSION: '8.0'
  
  # Performance targets
  TARGET_SSE_RELIABILITY: 99.5
  TARGET_DATABASE_IMPROVEMENT: 40
  TARGET_MEMORY_REDUCTION: 25
  OVERALL_PASS_THRESHOLD: 85
  
  # Test configuration
  ENABLE_PARALLEL_EXECUTION: ${{ inputs.parallel_execution || 'true' }}
  MAX_CONCURRENT_TESTS: 4
  RETRY_FAILED_TESTS: true
  MAX_RETRIES: 3

jobs:
  # =============================================================================
  # Pre-flight Checks and Setup
  # =============================================================================
  
  preflight:
    name: Pre-flight Checks
    runs-on: ubuntu-latest
    outputs:
      test-level: ${{ steps.determine-level.outputs.level }}
      should-run-stress: ${{ steps.determine-level.outputs.stress }}
      performance-baseline: ${{ steps.determine-level.outputs.baseline }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Determine test level
        id: determine-level
        run: |
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            echo "level=${{ inputs.test_level }}" >> $GITHUB_OUTPUT
            echo "baseline=${{ inputs.performance_baseline }}" >> $GITHUB_OUTPUT
          elif [ "${{ github.event_name }}" = "schedule" ]; then
            echo "level=stress" >> $GITHUB_OUTPUT
            echo "baseline=false" >> $GITHUB_OUTPUT
          elif [ "${{ github.event_name }}" = "push" ] && [ "${{ github.ref }}" = "refs/heads/main" ]; then
            echo "level=integration" >> $GITHUB_OUTPUT
            echo "baseline=false" >> $GITHUB_OUTPUT
          else
            echo "level=unit" >> $GITHUB_OUTPUT
            echo "baseline=false" >> $GITHUB_OUTPUT
          fi
          
          # Determine if stress tests should run
          if [ "${{ github.event_name }}" = "schedule" ] || [ "${{ inputs.test_level }}" = "stress" ] || [ "${{ inputs.test_level }}" = "full" ]; then
            echo "stress=true" >> $GITHUB_OUTPUT
          else
            echo "stress=false" >> $GITHUB_OUTPUT
          fi
      
      - name: Validate test configuration
        run: |
          echo "Test Level: ${{ steps.determine-level.outputs.level }}"
          echo "Run Stress Tests: ${{ steps.determine-level.outputs.stress }}"
          echo "Performance Baseline: ${{ steps.determine-level.outputs.baseline }}"
          echo "Parallel Execution: $ENABLE_PARALLEL_EXECUTION"
      
      - name: Check testing infrastructure
        run: |
          # Verify testing scripts exist
          required_files=(
            "scripts/testing/synthetic-domain-factory.js"
            "scripts/testing/performance-benchmarks.js"
            "scripts/testing/memory-monitor.js"
            "scripts/testing/run-performance-tests.sh"
            "scripts/testing/test-environment-setup.sh"
          )
          
          for file in "${required_files[@]}"; do
            if [ ! -f "$file" ]; then
              echo "❌ Missing required file: $file"
              exit 1
            else
              echo "✅ Found: $file"
            fi
          done
          
          echo "All required testing infrastructure files are present"

  # =============================================================================
  # Test Environment Setup
  # =============================================================================
  
  setup-environment:
    name: Setup Test Environment
    runs-on: ubuntu-latest
    needs: preflight
    
    services:
      mysql:
        image: mysql:8.0
        env:
          MYSQL_ROOT_PASSWORD: root_password
          MYSQL_DATABASE: performance_test
          MYSQL_USER: test_user
          MYSQL_PASSWORD: test_password
        ports:
          - 3306:3306
        options: >-
          --health-cmd="mysqladmin ping"
          --health-interval=10s
          --health-timeout=5s
          --health-retries=3
      
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd="redis-cli ping"
          --health-interval=10s
          --health-timeout=5s
          --health-retries=3
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: 'scripts/package.json'
      
      - name: Setup Go
        uses: actions/setup-go@v4
        with:
          go-version: ${{ env.GO_VERSION }}
          cache-dependency-path: 'backend/go.sum'
      
      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y mysql-client bc python3-pip
          pip3 install mysql-connector-python
      
      - name: Setup test environment
        run: |
          cd scripts/testing
          chmod +x test-environment-setup.sh
          chmod +x run-performance-tests.sh
          
          # Configure environment variables
          export TEST_DB_HOST=127.0.0.1
          export TEST_DB_PORT=3306
          export TEST_DB_USER=test_user
          export TEST_DB_PASSWORD=test_password
          export TEST_DB_NAME=performance_test
          export BACKEND_HOST=127.0.0.1
          export BACKEND_PORT=8080
          
          # Run environment setup
          ./test-environment-setup.sh --config-only
      
      - name: Install Node.js dependencies
        run: |
          cd scripts/testing
          if [ -f config/package.json ]; then
            cd config && npm install
          fi
      
      - name: Initialize database schema
        run: |
          # Wait for MySQL to be ready
          while ! mysqladmin ping -h127.0.0.1 -P3306 -utest_user -ptest_password --silent; do
            echo "Waiting for MySQL..."
            sleep 5
          done
          
          # Setup database schema
          mysql -h127.0.0.1 -P3306 -utest_user -ptest_password performance_test < scripts/testing/db-performance-test.sql
          echo "Database schema initialized"
      
      - name: Build backend service
        run: |
          cd backend
          go mod tidy
          go build -o apiserver cmd/apiserver/main.go
      
      - name: Start backend service
        run: |
          cd backend
          export DB_HOST=127.0.0.1
          export DB_PORT=3306
          export DB_USER=test_user
          export DB_PASSWORD=test_password
          export DB_NAME=performance_test
          
          ./apiserver &
          echo $! > apiserver.pid
          
          # Wait for service to start
          sleep 10
          
          # Health check
          for i in {1..30}; do
            if curl -s http://localhost:8080/health > /dev/null; then
              echo "Backend service is healthy"
              break
            fi
            echo "Waiting for backend service (attempt $i/30)..."
            sleep 2
          done
      
      - name: Upload environment state
        uses: actions/upload-artifact@v3
        with:
          name: test-environment
          path: |
            scripts/testing/config/
            backend/apiserver.pid
          retention-days: 1

  # =============================================================================
  # Unit Testing (Fast Feedback)
  # =============================================================================
  
  unit-tests:
    name: Unit Tests (1K domains)
    runs-on: ubuntu-latest
    needs: [preflight, setup-environment]
    if: needs.preflight.outputs.test-level == 'unit' || needs.preflight.outputs.test-level == 'integration' || needs.preflight.outputs.test-level == 'full'
    timeout-minutes: 5
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
      
      - name: Download environment state
        uses: actions/download-artifact@v3
        with:
          name: test-environment
          path: .
      
      - name: Install dependencies
        run: |
          cd scripts/testing
          npm install mysql2 uuid
      
      - name: Run unit tests
        run: |
          cd scripts/testing
          chmod +x run-performance-tests.sh
          
          # Set environment variables
          export DB_HOST=127.0.0.1
          export DB_PORT=3306
          export DB_USER=test_user
          export DB_PASSWORD=test_password
          export DB_NAME=performance_test
          
          # Run unit tests
          ./run-performance-tests.sh --unit --parallel
      
      - name: Upload unit test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: unit-test-results
          path: |
            scripts/testing/reports/
            scripts/testing/logs/
          retention-days: 7
      
      - name: Check unit test results
        run: |
          cd scripts/testing
          
          # Check if unit tests passed
          if [ -f "reports/pipeline_results/summary_*.txt" ]; then
            echo "Unit test summary:"
            cat reports/pipeline_results/summary_*.txt
            
            # Check overall result
            if grep -q "Overall Result: PASS" reports/pipeline_results/summary_*.txt; then
              echo "✅ Unit tests passed"
              exit 0
            else
              echo "❌ Unit tests failed"
              exit 1
            fi
          else
            echo "❌ No unit test results found"
            exit 1
          fi

  # =============================================================================
  # Integration Testing (Realistic Load)
  # =============================================================================
  
  integration-tests:
    name: Integration Tests (100K domains)
    runs-on: ubuntu-latest
    needs: [preflight, setup-environment, unit-tests]
    if: |
      always() && 
      needs.unit-tests.result == 'success' &&
      (needs.preflight.outputs.test-level == 'integration' || needs.preflight.outputs.test-level == 'full')
    timeout-minutes: 15
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
      
      - name: Download environment state
        uses: actions/download-artifact@v3
        with:
          name: test-environment
          path: .
      
      - name: Install dependencies
        run: |
          cd scripts/testing
          npm install mysql2 uuid
      
      - name: Run integration tests
        run: |
          cd scripts/testing
          
          # Set environment variables
          export DB_HOST=127.0.0.1
          export DB_PORT=3306
          export DB_USER=test_user
          export DB_PASSWORD=test_password
          export DB_NAME=performance_test
          
          # Run integration tests
          ./run-performance-tests.sh --integration --parallel
      
      - name: Upload integration test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: integration-test-results
          path: |
            scripts/testing/reports/
            scripts/testing/logs/
          retention-days: 7
      
      - name: Validate performance targets
        run: |
          cd scripts/testing
          
          # Check performance targets
          python3 <<'EOF'
          import json
          import glob
          import os
          import sys

          TARGET_SSE = float(os.getenv("TARGET_SSE_RELIABILITY", "99.5"))
          TARGET_DB = float(os.getenv("TARGET_DATABASE_IMPROVEMENT", "40"))
          TARGET_MEMORY = float(os.getenv("TARGET_MEMORY_REDUCTION", "25"))
          OVERALL_THRESHOLD = float(os.getenv("OVERALL_PASS_THRESHOLD", "85"))

          def extract_sse_reliability(payload):
              candidates = []
              for key in ("sseReliability", "realtimeReliability", "realtimeStream"):
                  value = payload.get(key)
                  if value is not None:
                      candidates.append(value)
              realtime_channels = payload.get("realtimeChannels")
              if isinstance(realtime_channels, dict):
                  sse_channel = realtime_channels.get("sse") or realtime_channels.get("eventSource")
                  if sse_channel is not None:
                      candidates.append(sse_channel)
              legacy_ws = payload.get("webSocketPerformance")
              if isinstance(legacy_ws, dict):
                  candidates.append(legacy_ws)
              for candidate in candidates:
                  if isinstance(candidate, (int, float)):
                      return float(candidate)
                  if isinstance(candidate, dict):
                      for field in ("sessionReliability", "uptime", "successRate", "reliability", "availability"):
                          value = candidate.get(field)
                          if isinstance(value, (int, float)):
                              return float(value)
              return None

          def check_performance_targets():
              results_files = glob.glob("reports/*_performance_*.json")

              if not results_files:
                  print("❌ No performance results found")
                  return False

              targets_met = 0
              total_targets = 0

              for file in results_files:
                  try:
                      with open(file, 'r') as f:
                          data = json.load(f)

                      reliability = extract_sse_reliability(data)
                      if reliability is not None:
                          total_targets += 1
                          if reliability >= TARGET_SSE:
                              print(f"✅ SSE reliability: {reliability:.2f}% (target: {TARGET_SSE:.2f}%)")
                              targets_met += 1
                          else:
                              print(f"❌ SSE reliability: {reliability:.2f}% (target: {TARGET_SSE:.2f}%)")
                      else:
                          print("⚠️  No SSE reliability metrics found; skipping target evaluation")

                      if 'databasePerformance' in data and 'improvement' in data['databasePerformance']:
                          improvement = data['databasePerformance']['improvement'].get('queryPerformance', 0)
                          total_targets += 1
                          if improvement >= TARGET_DB:
                              print(f"✅ Database improvement: {improvement}% (target: {TARGET_DB:.0f}%)")
                              targets_met += 1
                          else:
                              print(f"❌ Database improvement: {improvement}% (target: {TARGET_DB:.0f}%)")
                      else:
                          print("⚠️  No database performance metrics found; skipping target evaluation")

                      if 'memoryUsage' in data and 'improvement' in data['memoryUsage']:
                          reduction = data['memoryUsage']['improvement'].get('memoryReduction', 0)
                          total_targets += 1
                          if reduction >= TARGET_MEMORY:
                              print(f"✅ Memory reduction: {reduction}% (target: {TARGET_MEMORY:.0f}%)")
                              targets_met += 1
                          else:
                              print(f"❌ Memory reduction: {reduction}% (target: {TARGET_MEMORY:.0f}%)")
                      else:
                          print("⚠️  No memory usage metrics found; skipping target evaluation")

                  except Exception as e:
                      print(f"Error reading {file}: {e}")

              if total_targets == 0:
                  print("❌ No performance metrics were evaluated")
                  return False

              success_rate = (targets_met / total_targets) * 100
              print(f"\nPerformance targets achieved: {targets_met}/{total_targets} ({success_rate:.1f}%)")

              return success_rate >= OVERALL_THRESHOLD

          if __name__ == "__main__":
              if check_performance_targets():
                  print("✅ Integration tests performance validation passed")
                  sys.exit(0)
              else:
                  print("❌ Integration tests performance validation failed")
                  sys.exit(1)
          EOF

  # =============================================================================
  # Stress Testing (Enterprise Scale)
  # =============================================================================
  
  stress-tests:
    name: Stress Tests (1M+ domains)
    runs-on: ubuntu-latest
    needs: [preflight, setup-environment]
    if: needs.preflight.outputs.should-run-stress == 'true'
    timeout-minutes: 120
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
      
      - name: Download environment state
        uses: actions/download-artifact@v3
        with:
          name: test-environment
          path: .
      
      - name: Install dependencies
        run: |
          cd scripts/testing
          npm install mysql2 uuid
      
      - name: Run stress tests
        run: |
          cd scripts/testing
          
          # Set environment variables for stress testing
          export DB_HOST=127.0.0.1
          export DB_PORT=3306
          export DB_USER=test_user
          export DB_PASSWORD=test_password
          export DB_NAME=performance_test
          export ENABLE_PARALLEL_EXECUTION=true
          export MAX_CONCURRENT_TESTS=6
          
          # Run stress tests
          ./run-performance-tests.sh --stress --parallel
      
      - name: Upload stress test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: stress-test-results
          path: |
            scripts/testing/reports/
            scripts/testing/logs/
          retention-days: 14
      
      - name: Validate enterprise scale performance
        run: |
          cd scripts/testing
          
          # Validate stress test performance via SSE metrics
          python3 <<'EOF'
          import json
          import glob
          import os
          import sys

          TARGET_SSE = float(os.getenv("TARGET_SSE_RELIABILITY", "99.5"))
          TARGET_DB = float(os.getenv("TARGET_DATABASE_IMPROVEMENT", "40"))
          TARGET_MEMORY = float(os.getenv("TARGET_MEMORY_REDUCTION", "25"))
          OVERALL_THRESHOLD = float(os.getenv("OVERALL_PASS_THRESHOLD", "85"))

          def extract_sse_reliability(payload):
              if not isinstance(payload, dict):
                  return None

              candidates = []
              for key in ("sseReliability", "realtimeReliability", "realtimeStream"):
                  value = payload.get(key)
                  if value is not None:
                      candidates.append(value)

              realtime_channels = payload.get("realtimeChannels")
              if isinstance(realtime_channels, dict):
                  sse_channel = realtime_channels.get("sse") or realtime_channels.get("eventSource")
                  if sse_channel is not None:
                      candidates.append(sse_channel)

              legacy_ws = payload.get("webSocketPerformance")
              if isinstance(legacy_ws, dict):
                  candidates.append(legacy_ws)

              for candidate in candidates:
                  if isinstance(candidate, (int, float)):
                      return float(candidate)
                  if isinstance(candidate, dict):
                      for field in ("sessionReliability", "uptime", "successRate", "reliability", "availability"):
                          value = candidate.get(field)
                          if isinstance(value, (int, float)):
                              return float(value)
              return None

          def evaluate_stress_targets():
              reports = glob.glob("reports/*_performance_*.json")

              if not reports:
                  print("❌ No stress test reports found")
                  return False

              total_targets = 0
              achieved = 0

              for report_path in reports:
                  try:
                      with open(report_path, 'r') as report:
                          data = json.load(report)

                      reliability = extract_sse_reliability(data.get("sseMetrics") or data.get("realtimeSnapshot") or data)
                      if reliability is not None:
                          total_targets += 1
                          if reliability >= TARGET_SSE:
                              print(f"✅ SSE reliability: {reliability:.2f}% (target: {TARGET_SSE:.2f}%)")
                              achieved += 1
                          else:
                              print(f"❌ SSE reliability: {reliability:.2f}% (target: {TARGET_SSE:.2f}%)")
                      else:
                          print("⚠️  No SSE metrics found in stress report")

                      db_delta = (data.get("databasePerformance") or {}).get("improvement", {}).get("queryPerformance")
                      if isinstance(db_delta, (int, float)):
                          total_targets += 1
                          if db_delta >= TARGET_DB:
                              print(f"✅ Database throughput improvement: {db_delta}% (target: {TARGET_DB:.0f}%)")
                              achieved += 1
                          else:
                              print(f"❌ Database throughput improvement: {db_delta}% (target: {TARGET_DB:.0f}%)")

                      memory_delta = (data.get("memoryUsage") or {}).get("improvement", {}).get("memoryReduction")
                      if isinstance(memory_delta, (int, float)):
                          total_targets += 1
                          if memory_delta >= TARGET_MEMORY:
                              print(f"✅ Memory reduction: {memory_delta}% (target: {TARGET_MEMORY:.0f}%)")
                              achieved += 1
                          else:
                              print(f"❌ Memory reduction: {memory_delta}% (target: {TARGET_MEMORY:.0f}%)")

                  except Exception as err:
                      print(f"Error reading {report_path}: {err}")

              if total_targets == 0:
                  print("❌ No stress performance metrics evaluated")
                  return False

              success_rate = (achieved / total_targets) * 100
              print(f"\nStress targets achieved: {achieved}/{total_targets} ({success_rate:.1f}%)")
              return success_rate >= OVERALL_THRESHOLD

          if __name__ == "__main__":
              if evaluate_stress_targets():
                  print("✅ Stress tests performance validation passed")
                  sys.exit(0)
              else:
                  print("❌ Stress tests performance validation failed")
                  sys.exit(1)
          EOF

  # =============================================================================
  # Performance Baseline Management
  # =============================================================================
  
  baseline-management:
    name: Performance Baseline Management
    runs-on: ubuntu-latest
    needs: [preflight, integration-tests]
    if: |
      always() && 
      needs.integration-tests.result == 'success' &&
      needs.preflight.outputs.performance-baseline == 'true'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Download test results
        uses: actions/download-artifact@v3
        with:
          name: integration-test-results
          path: test-results/
      
      - name: Establish performance baseline
        run: |
          cd test-results
          
          # Create baseline from current results
          python3 <<'EOF'
          import json
          import glob
          import os
          import shutil
          import datetime

          TARGET_SSE = float(os.getenv("TARGET_SSE_RELIABILITY", "99.5"))
          TARGET_DB = float(os.getenv("TARGET_DATABASE_IMPROVEMENT", "40"))
          TARGET_MEMORY = float(os.getenv("TARGET_MEMORY_REDUCTION", "25"))

          def compute_reliability_metrics(metrics):
            if metrics is None:
              return None
            if isinstance(metrics, (int, float)):
              return float(metrics)
            if isinstance(metrics, dict):
              for field in ("sseReliability", "sessionReliability", "uptime", "availability", "successRate"):
                value = metrics.get(field)
                if isinstance(value, (int, float)):
                  return float(value)
            return None

          def should_update_baseline(current_metrics, baseline_metrics):
            if not baseline_metrics:
              return True

            current_sse = compute_reliability_metrics(current_metrics.get("sseMetrics") or current_metrics.get("realtimeSnapshot"))
            baseline_sse = compute_reliability_metrics(baseline_metrics.get("sseMetrics") or baseline_metrics.get("realtimeSnapshot"))
            if current_sse and (baseline_sse is None or current_sse > baseline_sse):
              return True

            current_db = (current_metrics.get("databasePerformance") or {}).get("improvement", {}).get("queryPerformance")
            baseline_db = (baseline_metrics.get("databasePerformance") or {}).get("improvement", {}).get("queryPerformance")
            if isinstance(current_db, (int, float)) and (baseline_db is None or current_db > baseline_db):
              return True

            current_mem = (current_metrics.get("memoryUsage") or {}).get("improvement", {}).get("memoryReduction")
            baseline_mem = (baseline_metrics.get("memoryUsage") or {}).get("improvement", {}).get("memoryReduction")
            if isinstance(current_mem, (int, float)) and (baseline_mem is None or current_mem > baseline_mem):
              return True

            return False

          def update_baseline():
            current_reports = sorted(glob.glob("reports/*_performance_*.json"))
            if not current_reports:
              print("No reports found - skipping baseline update")
              return

            latest_report = current_reports[-1]
            baselines_dir = "baselines"
            os.makedirs(baselines_dir, exist_ok=True)
            baseline_file = os.path.join(baselines_dir, "sse_performance_baseline.json")

            with open(latest_report, 'r') as f:
              current_metrics = json.load(f)

            current_sse = compute_reliability_metrics(current_metrics.get("sseMetrics") or current_metrics.get("realtimeSnapshot"))
            if current_sse is None or current_sse < TARGET_SSE:
              print(f"Skipping baseline update - current SSE reliability {current_sse}% below target {TARGET_SSE}%")
              return

            if os.path.exists(baseline_file):
              with open(baseline_file, 'r') as f:
                baseline_metrics = json.load(f)

              if not should_update_baseline(current_metrics, baseline_metrics):
                print("Existing baseline is better - no update needed")
                return

              timestamp = datetime.datetime.utcnow().strftime('%Y%m%d_%H%M%S')
              archive_file = os.path.join(baselines_dir, f"baseline_{timestamp}.json")
              shutil.copy2(baseline_file, archive_file)
              print(f"Archived previous baseline to {archive_file}")

            with open(baseline_file, 'w') as f:
              json.dump(current_metrics, f, indent=2)

            print(f"Updated SSE performance baseline with {latest_report}")

          if __name__ == "__main__":
            update_baseline()
          EOF
      
      - name: Upload baseline
        uses: actions/upload-artifact@v3
        with:
          name: performance-baseline
          path: test-results/performance-baseline.json
          retention-days: 30

  # =============================================================================
  # Test Results Aggregation and Reporting
  # =============================================================================
  
  aggregate-results:
    name: Aggregate Test Results
    runs-on: ubuntu-latest
    needs: [preflight, unit-tests, integration-tests, stress-tests]
    if: always()
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Download all test results
        uses: actions/download-artifact@v3
        with:
          path: all-results/
      
      - name: Generate comprehensive report
        run: |
          cd all-results
          
          # Create comprehensive test report
          python3 <<'EOF'
          import json
          import glob
          import os
          import sys
          from datetime import datetime

          TARGET_SSE = float(os.getenv("TARGET_SSE_RELIABILITY", "99.5"))
          TARGET_DB = float(os.getenv("TARGET_DATABASE_IMPROVEMENT", "40"))
          TARGET_MEMORY = float(os.getenv("TARGET_MEMORY_REDUCTION", "25"))

          def extract_sse_reliability(payload):
              if not isinstance(payload, dict):
                  return None

              candidates = []
              for key in ("sseReliability", "realtimeReliability", "realtimeStream"):
                  value = payload.get(key)
                  if value is not None:
                      candidates.append(value)

              realtime_channels = payload.get("realtimeChannels")
              if isinstance(realtime_channels, dict):
                  sse_channel = realtime_channels.get("sse") or realtime_channels.get("eventSource")
                  if sse_channel is not None:
                      candidates.append(sse_channel)

              legacy_ws = payload.get("webSocketPerformance")
              if isinstance(legacy_ws, dict):
                  candidates.append(legacy_ws)

              for candidate in candidates:
                  if isinstance(candidate, (int, float)):
                      return float(candidate)
                  if isinstance(candidate, dict):
                      for field in ("sessionReliability", "uptime", "successRate", "reliability", "availability"):
                          value = candidate.get(field)
                          if isinstance(value, (int, float)):
                              return float(value)
              return None

          def generate_report():
              report = {
                  "timestamp": datetime.utcnow().isoformat() + "Z",
                  "pipeline": {
                      "id": "${{ github.run_id }}",
                      "commit": "${{ github.sha }}",
                      "branch": "${{ github.ref_name }}",
                      "event": "${{ github.event_name }}",
                      "actor": "${{ github.actor }}"
                  },
                  "summary": {
                      "total_tests": 0,
                      "passed_tests": 0,
                      "failed_tests": 0,
                      "overall_result": "FAIL",
                      "performance_targets_met": 0,
                      "performance_targets_total": 3
                  },
                  "test_levels": {},
                  "performance_analysis": {
                      "sse_reliability": {"achieved": False, "value": 0.0, "target": TARGET_SSE},
                      "database_improvement": {"achieved": False, "value": 0.0, "target": TARGET_DB},
                      "memory_reduction": {"achieved": False, "value": 0.0, "target": TARGET_MEMORY}
                  },
                  "recommendations": []
              }

              for artifact_dir in os.listdir('.'):
                  if 'test-results' in artifact_dir:
                      artifact_path = os.path.join(artifact_dir, 'reports')
                      if os.path.exists(artifact_path):
                          process_test_results(artifact_path, report)

              if report["summary"]["total_tests"] > 0:
                  pass_rate = (report["summary"]["passed_tests"] / report["summary"]["total_tests"]) * 100
                  report["summary"]["pass_rate"] = round(pass_rate, 1)
                  report["summary"]["overall_result"] = "PASS" if pass_rate >= 85 else "FAIL"

              generate_recommendations(report)

              with open("comprehensive-test-report.json", "w") as f:
                  json.dump(report, f, indent=2)

              generate_markdown_summary(report)

              return report["summary"]["overall_result"] == "PASS"

          def process_test_results(results_path, report):
              for file in glob.glob(os.path.join(results_path, "*.json")):
                  try:
                      with open(file, 'r') as f:
                          data = json.load(f)

                      test_level = os.path.basename(file).split('_')[0]
                      report["test_levels"].setdefault(test_level, {"status": "unknown", "duration": 0, "metrics": {}})

                      process_performance_metrics(data, report)

                      report["summary"]["total_tests"] += 1
                      if "PASS" in str(data):
                          report["summary"]["passed_tests"] += 1
                      else:
                          report["summary"]["failed_tests"] += 1

                  except Exception as e:
                      print(f"Error processing {file}: {e}")

          def process_performance_metrics(data, report):
              reliability = extract_sse_reliability(data)
              perf = report["performance_analysis"]

              if reliability is not None and reliability > perf["sse_reliability"]["value"]:
                  perf["sse_reliability"]["value"] = reliability
                  prev_state = perf["sse_reliability"]["achieved"]
                  perf["sse_reliability"]["achieved"] = reliability >= TARGET_SSE
                  if perf["sse_reliability"]["achieved"] and not prev_state:
                      report["summary"]["performance_targets_met"] += 1

              if 'databasePerformance' in data and 'improvement' in data['databasePerformance']:
                  improvement = data['databasePerformance']['improvement'].get('queryPerformance', 0)
                  if improvement > perf["database_improvement"]["value"]:
                      perf["database_improvement"]["value"] = improvement
                      prev_state = perf["database_improvement"]["achieved"]
                      perf["database_improvement"]["achieved"] = improvement >= TARGET_DB
                      if perf["database_improvement"]["achieved"] and not prev_state:
                          report["summary"]["performance_targets_met"] += 1

              if 'memoryUsage' in data and 'improvement' in data['memoryUsage']:
                  reduction = data['memoryUsage']['improvement'].get('memoryReduction', 0)
                  if reduction > perf["memory_reduction"]["value"]:
                      perf["memory_reduction"]["value"] = reduction
                      prev_state = perf["memory_reduction"]["achieved"]
                      perf["memory_reduction"]["achieved"] = reduction >= TARGET_MEMORY
                      if perf["memory_reduction"]["achieved"] and not prev_state:
                          report["summary"]["performance_targets_met"] += 1

          def generate_recommendations(report):
              recommendations = []
              perf = report["performance_analysis"]

              if not perf["sse_reliability"]["achieved"]:
                  value = perf["sse_reliability"]["value"] or 0
                  recommendations.append({
                      "category": "SSE Reliability",
                      "priority": "Critical",
                      "issue": f"SSE reliability: {value:.2f}% (target: {TARGET_SSE:.2f}%)",
                      "action": "Analyze EventSource disconnects, ensure heartbeat intervals, and enable SSE retry backoff"
                  })

              if not perf["database_improvement"]["achieved"]:
                  recommendations.append({
                      "category": "Database Performance",
                      "priority": "High",
                      "issue": f"Database improvement: {perf['database_improvement']['value']:.1f}% (target: {TARGET_DB:.0f}%)",
                      "action": "Refine campaign aggregation queries and extend read replica coverage"
                  })

              if not perf["memory_reduction"]["achieved"]:
                  recommendations.append({
                      "category": "Memory Usage",
                      "priority": "Medium",
                      "issue": f"Memory reduction: {perf['memory_reduction']['value']:.1f}% (target: {TARGET_MEMORY:.0f}%)",
                      "action": "Adopt streaming reducers and aggressively reclaim campaign buffers"
                  })

              report["recommendations"] = recommendations

          def generate_markdown_summary(report):
              with open("test-summary.md", "w") as f:
                  f.write("# Enterprise-Scale Migration Testing Report\n\n")
                  f.write(f"**Pipeline ID:** {report['pipeline']['id']}\n")
                  f.write(f"**Commit:** {report['pipeline']['commit'][:8]}\n")
                  f.write(f"**Branch:** {report['pipeline']['branch']}\n")
                  f.write(f"**Timestamp:** {report['timestamp']}\n\n")

                  f.write("## Summary\n\n")
                  f.write(f"- **Overall Result:** {'✅ PASS' if report['summary']['overall_result'] == 'PASS' else '❌ FAIL'}\n")
                  f.write(f"- **Tests Executed:** {report['summary']['total_tests']}\n")
                  f.write(f"- **Pass Rate:** {report['summary'].get('pass_rate', 0):.1f}%\n")
                  f.write(f"- **Performance Targets Met:** {report['summary']['performance_targets_met']}/{report['summary']['performance_targets_total']}\n\n")

                  f.write("## Performance Targets\n\n")
                  for target_name, target_data in report["performance_analysis"].items():
                      status = "✅" if target_data["achieved"] else "❌"
                      target_label = target_name.replace("_", " ").title()
                      f.write(f"- **{target_label}:** {status} {target_data['value']:.1f}% (target: {target_data['target']:.1f}%)\n")

                  f.write("\n")

                  if report["recommendations"]:
                      f.write("## Recommendations\n\n")
                      for rec in report["recommendations"]:
                          f.write(f"### {rec['category']} ({rec['priority']} Priority)\n")
                          f.write(f"**Issue:** {rec['issue']}\n")
                          f.write(f"**Action:** {rec['action']}\n\n")

          if __name__ == "__main__":
              if generate_report():
                  print("✅ Comprehensive test report generated successfully")
                  sys.exit(0)
              else:
                  print("❌ Test results indicate failure")
                  sys.exit(1)
          EOF
      
      - name: Upload comprehensive report
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: comprehensive-test-report
          path: |
            all-results/comprehensive-test-report.json
            all-results/test-summary.md
          retention-days: 30
      
      - name: Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            
            try {
              const summary = fs.readFileSync('all-results/test-summary.md', 'utf8');
              
              await github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: summary
              });
            } catch (error) {
              console.log('Could not post PR comment:', error);
            }
      
      - name: Set job status
        run: |
          cd all-results
          
          if [ -f "comprehensive-test-report.json" ]; then
            overall_result=$(python3 -c "
import json
with open('comprehensive-test-report.json', 'r') as f:
    data = json.load(f)
print(data['summary']['overall_result'])
")
            
            if [ "$overall_result" = "PASS" ]; then
              echo "✅ All tests passed successfully"
              exit 0
            else
              echo "❌ Test execution failed"
              exit 1
            fi
          else
            echo "❌ No comprehensive report found"
            exit 1
          fi

  # =============================================================================
  # Notification and Cleanup
  # =============================================================================
  
  notify-completion:
    name: Notify Completion
    runs-on: ubuntu-latest
    needs: [aggregate-results]
    if: always()
    
    steps:
      - name: Determine notification status
        id: status
        run: |
          if [ "${{ needs.aggregate-results.result }}" = "success" ]; then
            echo "status=success" >> $GITHUB_OUTPUT
            echo "message=Enterprise-scale migration testing completed successfully" >> $GITHUB_OUTPUT
            echo "color=good" >> $GITHUB_OUTPUT
          else
            echo "status=failure" >> $GITHUB_OUTPUT
            echo "message=Enterprise-scale migration testing failed" >> $GITHUB_OUTPUT
            echo "color=danger" >> $GITHUB_OUTPUT
          fi
      
      - name: Post to Slack (if configured)
        if: env.SLACK_WEBHOOK_URL != ''
        uses: 8398a7/action-slack@v3
        with:
          status: ${{ steps.status.outputs.status }}
          channel: '#migration-testing'
          message: |
            ${{ steps.status.outputs.message }}
            Pipeline: ${{ github.run_id }}
            Commit: ${{ github.sha }}
            Branch: ${{ github.ref_name }}
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
      
      - name: Create GitHub issue on failure
        if: steps.status.outputs.status == 'failure' && github.event_name == 'schedule'
        uses: actions/github-script@v6
        with:
          script: |
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: `Nightly Migration Testing Failed - ${new Date().toISOString().split('T')[0]}`,
              body: `
                # Nightly Migration Testing Failure
                
                The scheduled enterprise-scale migration testing has failed.
                
                **Pipeline ID:** ${{ github.run_id }}
                **Commit:** ${{ github.sha }}
                **Timestamp:** ${new Date().toISOString()}
                
                Please check the [workflow run](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}) for details.
                
                ## Next Steps
                1. Review test results and logs
                2. Identify root cause of failures
                3. Implement fixes
                4. Re-run testing pipeline
                
                /cc @migration-team
              `,
              labels: ['testing', 'migration', 'failure', 'urgent']
            });
      
      - name: Summary
        run: |
          echo "==============================================="
          echo "Enterprise-Scale Migration Testing Complete"
          echo "==============================================="
          echo "Status: ${{ steps.status.outputs.status }}"
          echo "Pipeline ID: ${{ github.run_id }}"
          echo "Commit: ${{ github.sha }}"
          echo "Branch: ${{ github.ref_name }}"
          echo "Event: ${{ github.event_name }}"
          echo "==============================================="
